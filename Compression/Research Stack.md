| Model Compression                   | Progress |
| ----------------------------------- | -------- |
| Precision Format                    | DONE     |
| Previous survey on NN               |          |
| Survey potential research mentioned |          |
| Accuracy and zero shot in LLM       |          |
| Common benchmark                    |          |
| Datasets and framework mentioned    |          |
| Read quantization                   |          |
|                                     |          |

Câu hỏi:
1. Vì sao trong KD lại hay dùng reverse KL
2. Viết chi tiết các kỹ thuật, datasets sử dụng, kết quả: các metrics và inference time, thiết bị
   Với quantization: Thêm kỹ thuật quantize

## Paper to read:
### Quantization
#### 2022
- [x] ZeroQuant
- [x] LLM.int8()
#### 2023
- [x] GPTQ
- [x] LLM-Qat
- [ ] QuIP
- [ ] AWQ
- [ ] SqueezeLLM
- [ ] SmoothQuant
- [ ] RPTQ
- [ ] OliVe
- [ ] OS+
- [ ] LLM-FP4
#### 2024
- [x] BitDistiller
- [x] OneBit
- [ ] LUT-GEMM
- [ ] OWQ
- [ ] SpQR
- [ ] OmniQuant
- [ ] KVQuant
- [ ] KIVI
- [ ] WKVQuant